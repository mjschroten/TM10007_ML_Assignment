{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template -- ECG data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_VsRlKukSmc"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "lK44S6bKvDvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
        "\n",
        "from sklearn.feature_selection import f_classif, SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "## Classifiers\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "## Ensembling\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin"
      ],
      "metadata": {
        "id": "hWbJd3An9_NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiDn2Sk-VWqE"
      },
      "outputs": [],
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tm10007_ml/ecg')\n",
        "\n",
        "data = pd.read_csv('/content/tm10007_ml/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-processing**"
      ],
      "metadata": {
        "id": "zU0UNSLR9c0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separate labels"
      ],
      "metadata": {
        "id": "fRGWMzvjmWXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data has a 'label' column indicating the class\n",
        "label = data['label']\n",
        "\n",
        "# Separate data based on the label\n",
        "if sum(data['label']) > len(data) / 2:\n",
        "    normal_data = data[label == 0]\n",
        "    abnormal_data = data[label == 1]\n",
        "else:\n",
        "    normal_data = data[label == 1]\n",
        "    abnormal_data = data[label == 0]\n",
        "\n",
        "# Create data without the labels\n",
        "data_no_label = data.drop('label', axis=1)  # All features"
      ],
      "metadata": {
        "id": "AjjHQexaEFDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing data handling"
      ],
      "metadata": {
        "id": "5s9S-RYWmWIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Check for missing data\n",
        "# Check for any None values in data\n",
        "has_missing = data.isnull().values.any()\n",
        "print(f\"Missing values present? {has_missing}\")\n",
        "\n",
        "# Check for any zeros in data\n",
        "has_zeros = (data_no_label == 0).values.any()\n",
        "print(f\"Zero values present? {has_zeros}\")\n",
        "\n",
        "# Calculate total number of zeros\n",
        "total_zeros = (data_no_label == 0).sum().sum()\n",
        "print(f\"Total zeros in DataFrame: {total_zeros}\")\n",
        "\n",
        "##### Overview of where zeros are to decide missing data handling strategy\n",
        "# Count how many rows have at least one zero\n",
        "rows_with_zero = (data_no_label == 0).any(axis=1).sum()\n",
        "print(f\"Number of rows with at least one zero: {rows_with_zero}\")\n",
        "\n",
        "# Count how many columns have at least one zero\n",
        "columns_with_zero = (data_no_label == 0).any(axis=0).sum()\n",
        "print(f\"Number of columns with at least one zero: {columns_with_zero}\")\n",
        "\n",
        "# Create table with zero count for the rows\n",
        "zero_counts_per_row = (data_no_label == 0).sum(axis=1)\n",
        "zero_count_table = pd.DataFrame({'Row_Index': data_no_label.index, 'Zero_Count': zero_counts_per_row})\n",
        "zero_count_table.set_index('Row_Index', inplace=True)\n",
        "\n",
        "# Create table with zero count for the columns\n",
        "zero_counts_per_column = (data_no_label == 0).sum(axis=0)\n",
        "zero_count_table = pd.DataFrame({'Column_Name': zero_counts_per_column.index, 'Zero_Count': zero_counts_per_column.values})\n",
        "zero_count_table.set_index('Column_Name', inplace=True)\n",
        "\n",
        "##### Remove missing data\n",
        "# Remove rows with more than 10 zeros\n",
        "zero_counts_per_row = (data_no_label == 0).sum(axis=1)\n",
        "rows_to_keep = zero_counts_per_row[zero_counts_per_row <= 10].index\n",
        "filtered_data = data_no_label.loc[rows_to_keep]\n",
        "\n",
        "# Print removed rows with zeros\n",
        "data_with_zeros = data_no_label[(data_no_label == 0).any(axis=1)]\n",
        "zero_counts_per_row = (data_with_zeros == 0).sum(axis=1)\n",
        "print(zero_counts_per_row)\n",
        "\n",
        "# Check if all rows with zeros are removed\n",
        "rows_with_zero = (filtered_data == 0).any(axis=1).sum()\n",
        "print(f\"Number of rows with at least one zero (filtered data): {rows_with_zero}\")\n",
        "\n",
        "# Calculate total number of zeros to make sure all are removed from the whole dataframe\n",
        "total_zeros = (filtered_data == 0).sum().sum()\n",
        "print(f\"Total zeros in the DataFrame (excluding last column) after removing rows with zeros: {total_zeros}\")"
      ],
      "metadata": {
        "id": "G-phISsk9dJA",
        "outputId": "ae818e10-5da0-49a7-98ba-ae519c3ba73f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values present? False\n",
            "Zero values present? True\n",
            "Total zeros in DataFrame: 10500\n",
            "Number of rows with at least one zero: 14\n",
            "Number of columns with at least one zero: 4500\n",
            "177    750\n",
            "251    750\n",
            "269    750\n",
            "321    750\n",
            "323    750\n",
            "385    750\n",
            "434    750\n",
            "446    750\n",
            "537    750\n",
            "542    750\n",
            "575    750\n",
            "601    750\n",
            "784    750\n",
            "790    750\n",
            "dtype: int64\n",
            "Number of rows with at least one zero (filtered data): 0\n",
            "Total zeros in the DataFrame (excluding last column) after removing rows with zeros: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test data"
      ],
      "metadata": {
        "id": "RtmLVi359L4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing set (for final evaluation als dit nodig is)\n",
        "data_train, data_test, label_train, label_test = train_test_split(data_no_label, label, test_size=0.2, random_state=42, stratify=label)\n",
        "\n",
        "# Define K-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "V6pFCX3R9Mol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling: normalisation or standardisation"
      ],
      "metadata": {
        "id": "mL1Yg6Mz9n14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(data_no_label)\n",
        "data_standardized = pd.DataFrame(data_standardized, columns=data_no_label.columns, index=data_no_label.index)  # Convert back to DataFrame\n",
        "\n",
        "# Normalization\n",
        "scaler = MinMaxScaler()\n",
        "data_normalized = scaler.fit_transform(data_no_label)\n",
        "data_normalized = pd.DataFrame(data_normalized, columns=data_no_label.columns, index=data_no_label.index)  # Convert back to DataFrame\n",
        "\n",
        "# Decide to use standardization or normalization, based on performance metrics (accuracy, precision, recall)\n",
        "# For now start with standardization --> default choice"
      ],
      "metadata": {
        "id": "Y5gQVA3x9umI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature selection and extraction**"
      ],
      "metadata": {
        "id": "c3NGZOr1usBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Preliminary filtering using univariate statistical testing: ANOVA f-test\n",
        "selector = SelectKBest(f_classif, k=1000) # Select top 1000 features\n",
        "data_selected = selector.fit_transform(data_train, label_train) # Fit to the training data\n",
        "\n",
        "# Get the names of the top 1000 features\n",
        "selected_feature_indices = selector.get_support(indices=True)  # Get indices of selected features\n",
        "selected_feature_names = data_train.columns[selector.get_support()] # Data is pandas dataframe\n",
        "\n",
        "# Dataframe with selected features for training data\n",
        "data_selected = pd.DataFrame(data_selected, columns=selected_feature_names, index=data_train.index)\n",
        "\n",
        "print('Univariatiate statistical feature selection performed: 1000 features left.')\n",
        "\n",
        "# 2. Dimensionality reduction using PCA\n",
        "pca = PCA(n_components=100)  # Reduce to 100 features\n",
        "data_pca_selected = pca.fit_transform(data_selected) # Fit to the training data\n",
        "\n",
        "# Dataframe with PCA-transformed features for training data\n",
        "data_pca_selected = pd.DataFrame(data_pca_selected, index=data_selected.index)\n",
        "\n",
        "print('PCA feature selection performed: 100 features left.')\n",
        "\n",
        "# 3. Visualize new features with t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42) # Reduce to 2 dimensions for plotting\n",
        "data_tsne = tsne.fit_transform(data_pca_selected)\n",
        "\n",
        "# Create a scatter plot\n",
        "#plt.figure(figsize=(8, 6))\n",
        "#plt.scatter(data_tsne[label_train == 0, 0], data_tsne[label_train == 0, 1], label='Label 0', marker='o')  # Plot points for label 0\n",
        "#plt.scatter(data_tsne[label_train == 1, 0], data_tsne[label_train == 1, 1], label='Label 1', marker='x')  # Plot points for label 1\n",
        "#plt.legend()  # Add a legend to identify the labels\n",
        "#plt.title('t-SNE Visualization of Selected Features')\n",
        "#plt.xlabel('t-SNE Dimension 1')\n",
        "#plt.ylabel('t-SNE Dimension 2')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "6H6QYLK0up7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classify**"
      ],
      "metadata": {
        "id": "WTYzsg0xSUfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Try classifiers**"
      ],
      "metadata": {
        "id": "e_mWYQmLGLG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colorplot def"
      ],
      "metadata": {
        "id": "qXGb8CzmZdSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def colorplot(clf, ax, x, y, h=100):\n",
        "    '''\n",
        "    Overlay the decision areas as colors in an axes.\n",
        "\n",
        "    Input:\n",
        "        clf: trained classifier\n",
        "        ax: axis to overlay color mesh on\n",
        "        x: feature on x-axis\n",
        "        y: feature on y-axis\n",
        "        h(optional): steps in the mesh\n",
        "    '''\n",
        "    # Create a meshgrid the size of the axis\n",
        "    xstep = (x.max() - x.min() ) / 20.0\n",
        "    ystep = (y.max() - y.min() ) / 20.0\n",
        "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
        "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
        "    h = max((x_max - x_min, y_max - y_min))/h\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    if hasattr(clf, \"decision_function\"):\n",
        "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    else:\n",
        "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    if len(Z.shape) > 1:\n",
        "        Z = Z[:, 1]\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    cm = plt.cm.RdBu_r\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm"
      ],
      "metadata": {
        "id": "yB7Eaxk_ZbuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop over different classfiers"
      ],
      "metadata": {
        "id": "RNKTDHw0iWBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svmlin = SVC(kernel='linear', gamma='scale')\n",
        "svmrbf = SVC(kernel='rbf', gamma='scale')\n",
        "svmpoly = SVC(kernel='poly', degree=3, gamma='scale')\n",
        "svmsig = SVC(kernel='sigmoid', gamma='scale')\n",
        "\n",
        "clfs = [LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(),\n",
        "        LogisticRegression(), GaussianNB(), SGDClassifier(),\n",
        "        KNeighborsClassifier(), DecisionTreeClassifier(),\n",
        "        svmlin, svmrbf, svmpoly, svmsig]\n",
        "\n",
        "for clf in clfs:\n",
        "  start_time = time.time()\n",
        "  clf = clf.fit(data_train, label_train)\n",
        "  y_pred = clf.predict(data_train)\n",
        "  t = (\"Misclassified: %d / %d\" % ((label_train != y_pred).sum(), data_train.shape[0]))\n",
        "  end_time = time.time()\n",
        "  runtime = end_time - start_time\n",
        "  print(f\"Clf: {clf}, {t}\")\n",
        "  print(f\"Runtime: {runtime:.2f} seconds\")"
      ],
      "metadata": {
        "id": "MnnqPnbqgT_H",
        "outputId": "24e3ada9-c382-45f8-d891-15cc4a8c2f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1d99cf4d8f2a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Misclassified: %d / %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_train\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear"
      ],
      "metadata": {
        "id": "4WEiorBkSt1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda = lda.fit(data_train, label_train)\n",
        "y_pred_lda = lda.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_lda).sum(), data_train.shape[0]))\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "print(t)\n",
        "print(f\"Runtime: {runtime:.2f} seconds\")"
      ],
      "metadata": {
        "id": "ekrKhar-ZAKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quadratic"
      ],
      "metadata": {
        "id": "tuM_s8VNS28B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda = qda.fit(data_train, label_train)\n",
        "y_pred_qda = qda.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_qda).sum(), data_train.shape[0]))\n",
        "print(t)"
      ],
      "metadata": {
        "id": "GtUxvny3S-bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_qda_test = qda.predict(data_test)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_test != y_pred_qda_test).sum(), data_train.shape[0]))\n",
        "print(t)"
      ],
      "metadata": {
        "id": "dr6oCn-ji1Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tNN"
      ],
      "metadata": {
        "id": "xtpg4KpSS5ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "NN = KNeighborsClassifier(n_neighbors=1)\n",
        "NN = NN.fit(data_train, label_train)\n",
        "y_pred_nn = NN.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_nn).sum(), data_train.shape[0]))\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "print(t)\n",
        "print(f\"Runtime: {runtime:.2f} seconds\")"
      ],
      "metadata": {
        "id": "J4w82KypS_Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision tree"
      ],
      "metadata": {
        "id": "KtAaK13YS_6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "dt = DecisionTreeClassifier()\n",
        "dt = dt.fit(data_train, label_train)\n",
        "y_pred_dt = dt.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_dt).sum(), data_train.shape[0]))\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "print(t)\n",
        "print(f\"Runtime: {runtime:.2f} seconds\")"
      ],
      "metadata": {
        "id": "TADod5EHTDY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "1zWrp23-THeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "svm = SVC()\n",
        "svm = svm.fit(data_train, label_train)\n",
        "y_pred_svm = svm.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_svm).sum(), data_train.shape[0]))\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "print(t)\n",
        "print(f\"Runtime: {runtime:.2f} seconds\")"
      ],
      "metadata": {
        "id": "_r1Prolph7Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svmlin = SVC(kernel='linear', gamma='scale')\n",
        "svmrbf = SVC(kernel='rbf', gamma='scale')\n",
        "svmpoly = SVC(kernel='poly', degree=3, gamma='scale')\n",
        "svmsig = SVC(kernel='sigmoid', gamma='scale')\n",
        "\n",
        "clfs = [svmlin, svmrbf, svmpoly, svmsig]\n",
        "\n",
        "# Make plot without classifiers:\n",
        "num = 0\n",
        "fig = plt.figure(figsize=(15,15))\n",
        "\n",
        "for clf in clfs:\n",
        "  start_time = time.time()\n",
        "  clf = clf.fit(data_train, label_train)\n",
        "  y_pred = clf.predict(data_train)\n",
        "  t = (\"Misclassified: %d / %d\" % ((label_train != y_pred).sum(), data_train.shape[0]))\n",
        "  end_time = time.time()\n",
        "  runtime = end_time - start_time\n",
        "  print(f\"Clf: {clf}, {t}\")\n",
        "  print(f\"Runtime: {runtime:.2f} seconds\")\n",
        "\n",
        "  # For plotting, only works using 2 features\n",
        "  ax = fig.add_subplot(3, 2, num + 1)\n",
        "  x = data_train.iloc[:, 0]\n",
        "  y = data_train.iloc[:, 1]\n",
        "  ax.scatter(x, y, marker='o', c=label_train,\n",
        "      s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "  colorplot(clf, ax, x, y)\n",
        "  ax.set_title(f\"Clf: {clf}, {t}\")\n",
        "  num += 1"
      ],
      "metadata": {
        "id": "JtP642PYnetL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Colorplot only using two features to see what the SVC's do\n",
        "\n",
        "svmlin = SVC(kernel='linear', gamma='scale')\n",
        "svmrbf = SVC(kernel='rbf', gamma='scale')\n",
        "svmpoly = SVC(kernel='poly', degree=2, gamma='scale')\n",
        "svmsig = SVC(kernel='sigmoid', gamma='scale')\n",
        "\n",
        "clfs = [svmlin, svmrbf, svmpoly, svmsig]\n",
        "\n",
        "# # Make plot without classifiers:\n",
        "# num = 0\n",
        "# fig = plt.figure(figsize=(8,15))\n",
        "\n",
        "# Loop over classifiers\n",
        "for clf in clfs:\n",
        "  start_time = time.time()\n",
        "  x = data_train.iloc[:, 100:102]\n",
        "  clf = clf.fit(x, label_train)\n",
        "  y_pred = clf.predict(x)\n",
        "  t = (\"Misclassified: %d / %d\" % ((label_train != y_pred).sum(), data_train.shape[0]))\n",
        "  end_time = time.time()\n",
        "  runtime = end_time - start_time\n",
        "  print(f\"Clf: {clf}, {t}\")\n",
        "  print(f\"Runtime: {runtime:.2f} seconds\")\n",
        "\n",
        "  # # Plotting colorplot\n",
        "  # ax = fig.add_subplot(4, 1, num + 1)\n",
        "  # ax.scatter(x.iloc[:,0], x.iloc[:,1], marker='o', c=label_train,\n",
        "  #     s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "  # colorplot(clf, ax, x.iloc[:,0], x.iloc[:,1])\n",
        "  # ax.set_title(f\"Clf: {clf}, {t}\")\n",
        "  # num += 1"
      ],
      "metadata": {
        "id": "41gwKlOWoHwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "QoyFivsJS790"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_trees = [1, 5, 10, 50, 100]\n",
        "\n",
        "# # Make plot without classifiers:\n",
        "# num = 0\n",
        "# fig = plt.figure(figsize=(15,15))\n",
        "\n",
        "# Now use the classifiers on all datasets\n",
        "for n_tree in n_trees:\n",
        "  start_time = time.time()\n",
        "  rf = RandomForestClassifier(n_estimators=n_tree) # Om vast te zetten: random_state=42\n",
        "  rf.fit(data_train, label_train)\n",
        "  y_pred_rf = rf.predict(data_train)\n",
        "  t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_rf).sum(), data_train.shape[0]))\n",
        "  end_time = time.time()\n",
        "  runtime = end_time - start_time\n",
        "  print(f\"Tree: {n_tree}, {t}, Runtime: {runtime:.2f} seconds\")\n",
        "\n",
        "  ## For plotting, only works using 2 features\n",
        "  # ax = fig.add_subplot(3, 2, num + 1)\n",
        "  # x = data_train.iloc[:, 0]\n",
        "  # y = data_train.iloc[:, 1]\n",
        "  # ax.scatter(x, y, marker='o', c=label_train,\n",
        "  #     s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "  # colorplot(clf, ax, x, y)\n",
        "  # ax.set_title(f\"Tree: {n_tree}, {t}\")\n",
        "  # num += 1"
      ],
      "metadata": {
        "id": "BJgRc771THLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "7ZBzt9heTKIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb.fit(data_train, label_train)\n",
        "y_pred_XGB = clf.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_XGB).sum(), data_train.shape[0]))"
      ],
      "metadata": {
        "id": "ux4N2sCgTMJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_est = [1, 5, 10, 50, 100]\n",
        "\n",
        "for n in n_est:\n",
        "  start_time = time.time()\n",
        "  xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
        "  xgb.fit(data_train, label_train)\n",
        "  y_pred_rf = clf.predict(data_train)\n",
        "  t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_rf).sum(), data_train.shape[0]))\n",
        "  end_time = time.time()\n",
        "  runtime = end_time - start_time\n",
        "  print(f\"Tree: {n}, {t}, Runtime: {runtime:.2f} seconds\")"
      ],
      "metadata": {
        "id": "sno4fE6PdhEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensembling"
      ],
      "metadata": {
        "id": "bQxFbt3N4gQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Voting"
      ],
      "metadata": {
        "id": "2tTlcEH4qBnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_ensemble_soft = VotingClassifier(\n",
        "    estimators=[('KNN', KNeighborsClassifier()), ('tree', DecisionTreeClassifier()), ('rf', RandomForestClassifier())],\n",
        "    voting='soft')\n",
        "voting_ensemble_hard = VotingClassifier(\n",
        "    estimators=[('KNN', KNeighborsClassifier()), ('tree', DecisionTreeClassifier()), ('rf', RandomForestClassifier())],\n",
        "    voting='hard')\n",
        "\n",
        "ves = voting_ensemble_soft.fit(data_train, label_train)\n",
        "y_pred_ves = ves.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_ves).sum(), data_train.shape[0]))\n",
        "print(t)\n",
        "\n",
        "veh = voting_ensemble_hard.fit(data_train, label_train)\n",
        "y_pred_veh = veh.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_veh).sum(), data_train.shape[0]))\n",
        "print(t)\n"
      ],
      "metadata": {
        "id": "DIbaSNS04o1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Averaging"
      ],
      "metadata": {
        "id": "nPW8Y2mpqEvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AveragingClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, estimators):\n",
        "        self.estimators = estimators\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for name, estimator in self.estimators: # Unpack the tuple into name and estimator\n",
        "            estimator.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Get probability predictions from each classifier\n",
        "        proba = np.stack([estimator.predict_proba(X) for name, estimator in self.estimators])\n",
        "\n",
        "        # Average the probabilities\n",
        "        averaged_proba = np.mean(proba, axis=0)\n",
        "        return averaged_proba\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Get class predictions based on averaged probabilities\n",
        "        averaged_proba = self.predict_proba(X)\n",
        "        predictions = np.argmax(averaged_proba, axis=1)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "7dUuvOaXqHP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averaging = AveragingClassifier(\n",
        "    estimators=[('KNN', KNeighborsClassifier()), ('tree', DecisionTreeClassifier()), ('rf', RandomForestClassifier())])\n",
        "averaging = averaging.fit(data_train, label_train)\n",
        "y_pred_averaging = averaging.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_averaging).sum(), data_train.shape[0]))\n",
        "print(t)"
      ],
      "metadata": {
        "id": "RCirbC4Vvc0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking (duurt ~1 min om te runnen)"
      ],
      "metadata": {
        "id": "dOAP9XjAqDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking allows to use the strength of each individual estimator by using\n",
        "# their output as input of a final estimator\n",
        "\n",
        "stacking = StackingClassifier(\n",
        "    estimators=[('KNN', KNeighborsClassifier()), ('tree', DecisionTreeClassifier()), ('rf', RandomForestClassifier())],\n",
        "    )\n",
        "stacking = stacking.fit(data_train, label_train)\n",
        "y_pred_stacking = stacking.predict(data_train)\n",
        "t = (\"Misclassified: %d / %d\" % ((label_train != y_pred_stacking).sum(), data_train.shape[0]))\n",
        "print(t)"
      ],
      "metadata": {
        "id": "qDdYF29oqL-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selection + voting/averaging"
      ],
      "metadata": {
        "id": "Kp0YMXoXqOqw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAmOcVjxqR0T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}