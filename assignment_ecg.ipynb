{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template -- ECG data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "lK44S6bKvDvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from scipy.stats import normaltest\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
        "\n",
        "from sklearn.feature_selection import f_classif, SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "id": "hWbJd3An9_NV"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_VsRlKukSmc"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "CiDn2Sk-VWqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86546144-f5f3-4c37-f149-90b830267f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n",
            "The number of samples: 827\n",
            "The number of columns: 9001\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "\n",
        "with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tm10007_ml/ecg')\n",
        "\n",
        "data = pd.read_csv('/content/tm10007_ml/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outer cross-validation\n",
        "Split data set intro training and test set"
      ],
      "metadata": {
        "id": "1BwFjEOGWxm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = data['label']\n",
        "\n",
        "# K-fold outer cross-validation\n",
        "k_folds = 1\n",
        "cv_outer = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42) #random_state ensures same shuffling pattern\n",
        "\n",
        "# Loop per training set\n",
        "for train1_index, test_index in cv_outer.split(data, label):\n",
        "    # Split data into first train and test sets\n",
        "    X_train1, X_test = data.iloc[train1_index], data.iloc[test_index]\n",
        "    y_train1, y_test = data.iloc[train1_index], data.iloc[test_index]\n",
        "\n",
        "    ### Pre-processing\n",
        "    ## Separate labels\n",
        "    # Check which label is normal and abnormal\n",
        "    if sum(data['label']) > len(data) / 2: # Biggest class is 'normal'\n",
        "        normal_data = data[label == 0]\n",
        "        abnormal_data = data[label == 1]\n",
        "    else:\n",
        "        normal_data = data[label == 1]\n",
        "        abnormal_data = data[label == 0]\n",
        "\n",
        "    # Split train data into label and data\n",
        "    label_train = X_train1['label']\n",
        "    data_train = X_train1.drop('label', axis=1)\n",
        "\n",
        "    ## Missing data handling\n",
        "    # Change None into 0\n",
        "    data_train_clean = data_train.copy()\n",
        "    imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
        "    data_train_clean[:] = imputer.fit_transform(data_train_clean)\n",
        "\n",
        "    # Check how many zeros are in the data per row\n",
        "    zero_counts_per_row = (data_train == 0).sum(axis=1)\n",
        "\n",
        "    # Remove rows with more than 5% zeros\n",
        "    threshold = 0.05 * data_train.shape[1] # threshold=5% of total columns\n",
        "    rows_to_keep = zero_counts_per_row[zero_counts_per_row <= threshold].index\n",
        "    filtered_data_train = data_train.loc[rows_to_keep]\n",
        "    filtered_label_train = label_train[label_train.index.isin(rows_to_keep)].reset_index(drop=True)\n",
        "\n",
        "    ## Scaling\n",
        "    # Check for normal distribution of features\n",
        "    p_values = filtered_data_train.apply(lambda col: normaltest(col)[1])\n",
        "    non_normal_features = (p_values < 0.05).sum() # a p-value > 0.05 means it is not normally distributed\n",
        "\n",
        "    # Find outliers in data\n",
        "    def count_outliers(df):\n",
        "      '''function to detect outliers in data'''\n",
        "      Q1 = df.quantile(0.25)\n",
        "      Q3 = df.quantile(0.75)\n",
        "      IQR = Q3 - Q1\n",
        "      lower_bound = Q1 - 1.5 * IQR\n",
        "      upper_bound = Q3 + 1.5 * IQR\n",
        "      outliers = ((df < lower_bound) | (df > upper_bound)).sum()\n",
        "      return outliers\n",
        "\n",
        "    outlier_counts = count_outliers(filtered_data_train)\n",
        "    mean_outliers = outlier_counts.mean()\n",
        "    features_with_many_outliers = (outlier_counts > 0.1 * len(filtered_data_train.index)).sum()\n",
        "\n",
        "    # Robust scaler chosen: based on normal distribution and outliers\n",
        "    scaler = RobustScaler()\n",
        "\n",
        "    ## Inner cross-validation\n",
        "    n_folds = 2\n",
        "    cv_inner = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42) #shuffle ensures no bias by ordered data\n",
        "\n",
        "    ## Classifiers\n",
        "    classifiers = {\n",
        "    \"LogisticRegression\": LogisticRegression(),\n",
        "    \"SGDClassifier\": SGDClassifier(),\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
        "    \"SVC\": SVC(),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier()}\n",
        "\n",
        "    # Hyperparameters per classifier\n",
        "    param_grids = {\n",
        "    \"SGDClassifier\": {\"loss\": ['hinge', 'log_loss', 'modified_huber'], \"penalty\": ['l2', 'l1', 'elasticnet'],\n",
        "                      \"alpha\": [0.001, 0.01, 0.1]},\n",
        "    \"KNeighborsClassifier\": {\"n_neighbors\": [2, 3, 5, 7, 9, 11, 15], \"weights\": ['uniform', 'distance']},\n",
        "    \"SVC\": {\"kernel\": ['linear', 'rbf', 'poly', 'sigmoid'], \"C\": [0.1, 1, 10],\n",
        "            \"coef0\": [0.0, 0.15, 0.3]},\n",
        "    \"RandomForestClassifier\": {\"n_estimators\": [1, 5, 10, 50, 100, 200], \"max_depth\": [3, 5, 7, 9, 11, 15]}}\n",
        "\n",
        "    auc_scores = {}\n",
        "    f1_scores = {}\n",
        "    ensemble_classifiers = []\n",
        "    f1_threshold = 0.35\n",
        "\n",
        "    ## Loop per cross-validation set\n",
        "    for train_index, val_index in cv_inner.split(filtered_data_train, filtered_label_train):\n",
        "        # Split data into train and validation sets\n",
        "        X_train, X_val = filtered_data_train.iloc[train_index], filtered_data_train.iloc[val_index]\n",
        "        y_train, y_val = filtered_label_train.iloc[train_index], filtered_label_train.iloc[val_index]\n",
        "\n",
        "        ## Scaling\n",
        "        scaler = RobustScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "        ## Feature selection\n",
        "        # 1. Univariate statistical testing: ANOVO f-test\n",
        "        # Number of features where p < 0.05\n",
        "        f_scores, p_values = f_classif(X_train_scaled, y_train)\n",
        "        k_best = np.sum(p_values < 0.05) # Only features with p_value < 0.05\n",
        "\n",
        "        selector = SelectKBest(f_classif, k=k_best)\n",
        "        X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "        X_val_selected = selector.transform(X_val_scaled)\n",
        "        print(f'Univariatiate statistical feature selection performed: {k_best} features selected.')\n",
        "\n",
        "        # 2. Dimensionality reduction with PCA\n",
        "        # Number of components where variance â‰¥90%\n",
        "        pca_full = PCA().fit(X_train_selected)\n",
        "        explained_var = pca_full.explained_variance_ratio_\n",
        "        cumulative_var = np.cumsum(explained_var)\n",
        "        pca_components = np.argmax(cumulative_var >= 0.90) + 1\n",
        "\n",
        "        pca = PCA(n_components=pca_components) # Reduce to features so that variance > 0.9\n",
        "        X_train_final_selected = pca.fit_transform(X_train_selected)\n",
        "        X_val_final_selected = pca.transform(X_val_selected)\n",
        "        print(f'PCA feature selection performed: {pca_components} features left.')\n",
        "\n",
        "        # Store classifiers for ensembling\n",
        "        fold_classifiers = []\n",
        "\n",
        "        # Create learning curve plots\n",
        "        i=0\n",
        "        fig, axes = plt.subplots(2, len(classifiers), figsize=(20, 5))\n",
        "\n",
        "        ## Loop per classifier\n",
        "        for clf_name, clf in classifiers.items():\n",
        "          auc_scores.setdefault(clf_name, [])\n",
        "          auc_scores.setdefault('VotingEnsembleHard', [])\n",
        "          f1_scores.setdefault(clf_name, [])\n",
        "          f1_scores.setdefault('VotingEnsembleHard', [])\n",
        "\n",
        "          ## Hyperparameter optimization\n",
        "          if clf_name in param_grids:\n",
        "                grid_search = GridSearchCV(clf, param_grids[clf_name], cv=cv, n_jobs=-1, scoring='f1', verbose=1)\n",
        "                grid_search.fit(X_train_final_selected, y_train)\n",
        "                clf = grid_search.best_estimator_\n",
        "                print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
        "          else:\n",
        "                clf.fit(X_train_final_selected, y_train)\n",
        "\n",
        "          ## Calculate AUC-score\n",
        "          y_pred = clf.predict(X_val_final_selected)\n",
        "          if hasattr(clf, \"predict_proba\"):\n",
        "            y_score = clf.predict_proba(X_val_final_selected)[:, 1]\n",
        "            auc = metrics.roc_auc_score(y_val, y_score)\n",
        "          else:\n",
        "                try:\n",
        "                    y_score = clf.decision_function(X_val_final_selected)\n",
        "                    auc = metrics.roc_auc_score(y_val, y_score)\n",
        "                except AttributeError:\n",
        "                    y_score = y_pred\n",
        "                    auc = metrics.roc_auc_score(y_val, y_score)\n",
        "\n",
        "          auc_scores[clf_name].append(auc)  # Store AUC for this classifier\n",
        "\n",
        "          # Calculate ROC\n",
        "          fpr, tpr, thresholds = roc_curve(y_val, y_score)\n",
        "\n",
        "          # Plot ROC curve for each classifier\n",
        "          axes[0,i].plot(fpr, tpr, label=f'{clf_name} (AUC = {auc:.2f})')\n",
        "          axes[0,i].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
        "          axes[0,i].set_xlabel('False Positive Rate')\n",
        "          axes[0,i].set_ylabel('True Positive Rate')\n",
        "          axes[0,i].set_title(f'ROC Curve - {clf_name}')\n",
        "          axes[0,i].legend()\n",
        "\n",
        "          ## Calculate F1-score for this classifier\n",
        "          f1 = metrics.f1_score(y_val, y_pred)\n",
        "          f1_scores[clf_name].append(f1)  # Store F1-score for this classifier\n",
        "\n",
        "          ## Learning curves\n",
        "          train_sizes, train_scores, val_scores = \\\n",
        "            learning_curve(clf, X_train_final_selected, y_train,\n",
        "                            cv=cv,n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "          train_scores_mean = np.mean(train_scores, axis=1)\n",
        "          train_scores_std = np.std(train_scores, axis=1)\n",
        "          val_scores_mean = np.mean(val_scores, axis=1)\n",
        "          val_scores_std = np.std(val_scores, axis=1)\n",
        "\n",
        "          axes[1,i].set_title(clf_name)\n",
        "          axes[1,i].set_ylim(0.3, 1.01)  # Adjust ylim as needed\n",
        "          axes[1,i].set_xlabel(\"Training examples\")\n",
        "          axes[1,i].set_ylabel(\"Score\")\n",
        "          axes[1,i].grid()\n",
        "          axes[1,i].fill_between(train_sizes,train_scores_mean - train_scores_std,\n",
        "                              train_scores_mean + train_scores_std,alpha=0.1,color=\"r\")\n",
        "          axes[1,i].fill_between(train_sizes,val_scores_mean - val_scores_std,val_scores_mean + val_scores_std,alpha=0.1,color=\"g\")\n",
        "          axes[1,i].plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\")\n",
        "          axes[1,i].plot(train_sizes, val_scores_mean, \"o-\", color=\"g\",\n",
        "                      label=\"Cross-validation score\")\n",
        "          axes[1,i].legend(loc=\"best\")\n",
        "\n",
        "          i += 1\n",
        "\n",
        "          # Selection for ensembling based on f1-score value\n",
        "          if f1 > f1_threshold: # Store high scoring classifiers\n",
        "            fold_classifiers.append((clf_name, clf))\n",
        "\n",
        "        # Out of classifier loop, still in cross-validation loop\n",
        "        ## Ensembling\n",
        "        if len(fold_classifiers) > 1:  # Cannot ensemble 1 classifier\n",
        "          voting_ensemble = VotingClassifier(estimators=fold_classifiers, voting='hard')\n",
        "          voting_ensemble.fit(X_train_final_selected, y_train)\n",
        "\n",
        "          # Calculate F1-score for ensemble\n",
        "          y_pred = voting_ensemble.predict(X_val_final_selected)\n",
        "          f1 = metrics.f1_score(y_val, y_pred)\n",
        "          f1_scores['VotingEnsembleHard'].append(f1)\n",
        "          ensemble_classifiers.append(voting_ensemble)\n",
        "        else:\n",
        "          print(f'Only {fold_classifiers} above selection threshold, no ensembling possible')\n",
        "\n",
        "    # Out of cross-validation loop\n",
        "    ## Select best ensembling model\n",
        "    best_f1 = max(f1_scores['VotingEnsembleHard'])  # Get the maximum value\n",
        "    best_idx = f1_scores['VotingEnsembleHard'].index(best_f1)  # Get the index of the maximum value\n",
        "    best_ensemble = ensemble_classifiers[best_idx]\n",
        "    print(f'Best ensemble scores {best_f1}, ensemble contains {best_ensemble}.')\n",
        "\n",
        "    ## Evaluate model on test data\n",
        "    # Change None into 0\n",
        "    data_test_clean = X_test.copy()\n",
        "    data_test_clean[:] = imputer.transform(data_test_clean)\n",
        "\n",
        "    # Remove rows with more than 10 zeros\n",
        "    zero_counts_per_row = (X_test == 0).sum(axis=1)\n",
        "    rows_to_keep = zero_counts_per_row[zero_counts_per_row <= threshold].index\n",
        "    filtered_data_test = X_test.loc[rows_to_keep]\n",
        "    filtered_label_test = y_test[y_test.index.isin(rows_to_keep)].reset_index(drop=True)\n",
        "\n",
        "    # Scaling testdata\n",
        "    X_test_scaled = scaler.transform(filtered_data_test)\n",
        "\n",
        "    # Feature selection\n",
        "    # 1. Univariate feature selection\n",
        "    X_test_selected = selector.transform(X_test_scaled)\n",
        "    # 2. PCA\n",
        "    X_test_final_selected = pca.transform(X_test_selected)\n",
        "\n",
        "    # Ensembling\n",
        "    y_pred_test = best_ensemble.predict(X_test_final_selected)\n",
        "    f1_test = metrics.f1_score(filtered_label_test, y_pred_test)\n",
        "    print(f\"F1-score on testdata: {f1_test}\")"
      ],
      "metadata": {
        "id": "V6pFCX3R9Mol"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Out of outer cross-validation loop\n",
        "Visualisation"
      ],
      "metadata": {
        "id": "0C4_0LqrzHEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize f-scores for univariate feature selection\n",
        "sorted_scores = np.sort(f_scores)[::-1]  # Sort on descending\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(sorted_scores[:2000])\n",
        "plt.axvline(x=k_best, color='r', linestyle='--', label=f'{k_best} features selected')\n",
        "plt.title('Sorted ANOVA F-scores of features')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('F-score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot variance for PCA\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(cumulative_var, marker='o', label='Cumulative variance')\n",
        "plt.axhline(y=0.90, color='r', linestyle='--', label='90%')\n",
        "plt.axvline(x=pca_components, color='g', linestyle='--', label=f'{pca_components} components')\n",
        "plt.title('Cumulative variance per PCA-component')\n",
        "plt.xlabel('Number of PCA-components')\n",
        "plt.ylabel('Cumulative variance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. Visualize new features with t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42) # Reduce to 2 dimensions for plotting\n",
        "data_tsne = tsne.fit_transform(data_pca_selected)\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data_tsne[filtered_label_train == 0, 0], data_tsne[filtered_label_train == 0, 1], label='Label 0', marker='o')  # Plot points for label 0\n",
        "plt.scatter(data_tsne[filtered_label_train == 1, 0], data_tsne[filtered_label_train == 1, 1], label='Label 1', marker='x')  # Plot points for label 1\n",
        "plt.legend()  # Add a legend to identify the labels\n",
        "plt.title('t-SNE Visualization of Selected Features')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SyMaRFaszPgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance estimate"
      ],
      "metadata": {
        "id": "w_JSeS85zQAC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxm-PzFnzeav"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}