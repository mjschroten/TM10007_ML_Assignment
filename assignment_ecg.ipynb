{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template -- ECG data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "lK44S6bKvDvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from statistics import median\n",
        "from scipy.stats import normaltest\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "\n",
        "from sklearn.feature_selection import f_classif, SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve,  average_precision_score, PrecisionRecallDisplay, ConfusionMatrixDisplay, f1_score, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n"
      ],
      "metadata": {
        "id": "hWbJd3An9_NV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_VsRlKukSmc"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CiDn2Sk-VWqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff0c464-d08e-45c7-8bd9-bd507ca06ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n",
            "The number of samples: 827\n",
            "The number of columns: 9001\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "\n",
        "with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tm10007_ml/ecg')\n",
        "\n",
        "data = pd.read_csv('/content/tm10007_ml/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nested cross-validation loop"
      ],
      "metadata": {
        "id": "1BwFjEOGWxm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SET UP ---\n",
        "# Split data into label and data\n",
        "label = data['label']\n",
        "data_nolabel = data.drop('label', axis=1)\n",
        "\n",
        "# K-fold outer cross-validation\n",
        "k_folds = 5\n",
        "cv_outer = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42) #random_state ensures same shuffling pattern\n",
        "\n",
        "# K-fold inner cross-validation\n",
        "n_folds = 5\n",
        "cv_inner = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42) #shuffle ensures no bias by ordered data\n",
        "\n",
        "# Classifiers\n",
        "classifiers = {\n",
        "    \"LogisticRegression\": LogisticRegression(class_weight='balanced', random_state=42), # class weight balanced adjusts the weights of the classes because classes are unbalanced\n",
        "    \"SGDClassifier\": SGDClassifier(class_weight='balanced', random_state=42),\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
        "    \"SVC\": SVC(class_weight='balanced', random_state=42),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(class_weight='balanced', random_state=42)}\n",
        "\n",
        "# Hyperparameters per classifier\n",
        "param_grids = {\n",
        "    \"LogisticRegression\": [\n",
        "        {\"C\": [0.01, 0.1, 1, 10], # Regularization strength\n",
        "         \"penalty\": ['l2'], # Regularization type\n",
        "         \"solver\": ['lbfgs']}, # Optimization algorithm\n",
        "        {\"C\": [0.01, 0.1, 1, 10],\n",
        "         \"penalty\": ['l2', 'l1'],\n",
        "         \"solver\": ['liblinear']}],\n",
        "    \"SGDClassifier\": {\n",
        "        \"loss\": ['hinge', 'log_loss', 'modified_huber'],  # Loss function\n",
        "        \"penalty\": ['l2', 'l1'],  # Regularization type\n",
        "        \"alpha\": [0.001, 0.01, 0.1]},  # Regularization amount\n",
        "    \"KNeighborsClassifier\": {\n",
        "        \"n_neighbors\": [2, 3, 5, 7, 9],  # Number of neighbors\n",
        "        \"weights\": ['uniform', 'distance']},  # Weighting scheme\n",
        "    \"DecisionTreeClassifier\": {\n",
        "        \"max_depth\": [3, 5, 7, 9, 11],  # Tree depth\n",
        "        \"min_samples_split\": [2, 5, 10, 15]},  # Minimum samples for split\n",
        "    \"SVC\": {\n",
        "        \"kernel\": ['linear', 'rbf', 'poly'],  # Kernel type\n",
        "        \"degree\": [2, 3, 5],  # Polynomial degree\n",
        "        \"C\": [0.1, 1, 10]},  # Regularization parameter\n",
        "    \"RandomForestClassifier\": {\n",
        "        \"n_estimators\": [1, 5, 10, 50],  # Number of trees\n",
        "        \"max_depth\": [3, 5, 7, 9, 11]}}  # Tree depth\n",
        "\n",
        "f1_threshold = 0.5\n",
        "f1_test = []\n",
        "precision_recall_list = []\n",
        "fold_num = 1\n",
        "\n",
        "# --- OUTER cross validation loop ---\n",
        "for train1_index, test_index in cv_outer.split(data_nolabel, label):\n",
        "    print(f\"Starting outer fold {fold_num}\")\n",
        "    fold_num += 1\n",
        "\n",
        "    # Split data into first train and test sets\n",
        "    X_train1, X_test = data_nolabel.iloc[train1_index], data_nolabel.iloc[test_index]\n",
        "    y_train1, y_test = label.iloc[train1_index], label.iloc[test_index]\n",
        "\n",
        "    ### Pre-processing\n",
        "    ## data checks\n",
        "    # Check which label is normal and abnormal\n",
        "    if sum(data['label']) > len(data) / 2: # Biggest class is 'normal'\n",
        "        normal_label = 0\n",
        "        abnormal_label = 1\n",
        "    else:\n",
        "        # Biggest class is 'abnormal'\n",
        "        normal_label = 1\n",
        "        abnormal_label = 0\n",
        "\n",
        "    # Check for normal distribution of features\n",
        "    p_values = X_train1.apply(lambda col: normaltest(col)[1]) # test normality of each feature and get p-value\n",
        "    non_normal_features = (p_values < 0.05).sum() # count number of features with p-value < 0.05 (not normally distributed)\n",
        "\n",
        "    ## Missing data handling\n",
        "    # Change None into median of training data\n",
        "    data_train_clean = X_train1.copy()\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    data_train_clean[:] = imputer.fit_transform(data_train_clean)\n",
        "\n",
        "    ## Scaling\n",
        "    # Find outliers in data\n",
        "    def count_outliers(df):\n",
        "      '''function to detect outliers in data'''\n",
        "      Q1 = df.quantile(0.25)\n",
        "      Q3 = df.quantile(0.75)\n",
        "      IQR = Q3 - Q1 # Spread of the middle 50% of data\n",
        "      lower_bound = Q1 - 1.5 * IQR\n",
        "      upper_bound = Q3 + 1.5 * IQR\n",
        "      outliers = ((df < lower_bound) | (df > upper_bound)).sum()\n",
        "      return outliers\n",
        "\n",
        "    outlier_counts = count_outliers(data_train_clean)\n",
        "    mean_outliers = outlier_counts.mean()\n",
        "    features_with_many_outliers = (outlier_counts > 0.1 * len(data_train_clean.index)).sum()\n",
        "\n",
        "    # MinMaxScaler chosen\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data_train_clean)\n",
        "\n",
        "    ## Feature selection\n",
        "    # 1. Univariate statistical testing: ANOVO f-test\n",
        "    # Number of features where p < 0.05\n",
        "    f_scores, p_values = f_classif(data_scaled, y_train1)\n",
        "    k_best = np.sum(p_values < 0.05) # Only features with p_value < 0.05\n",
        "\n",
        "    selector = SelectKBest(f_classif, k=k_best)\n",
        "    data_selected = selector.fit_transform(data_scaled, y_train1)\n",
        "    print(f'Univariatiate statistical feature selection performed: {k_best} features selected.')\n",
        "\n",
        "    # 2. Dimensionality reduction with PCA\n",
        "    # Number of components where variance ≥99%\n",
        "    pca_full = PCA().fit(data_selected)\n",
        "    explained_var = pca_full.explained_variance_ratio_\n",
        "    cumulative_var = np.cumsum(explained_var)\n",
        "    pca_components = np.argmax(cumulative_var >= 0.99) + 1   # find number of components that explain at least 99% of the variance\n",
        "\n",
        "    pca = PCA(n_components=pca_components) # Reduce to features so that variance > 0.99\n",
        "    data_final_selected = pca.fit_transform(data_selected)\n",
        "    print(f'PCA feature selection performed: {pca_components} features left.')\n",
        "\n",
        "    # initialize for storage\n",
        "    clf_params = {}\n",
        "    auc_scores = {}\n",
        "    f1_scores = {}\n",
        "    ensemble_classifiers = []\n",
        "    ensemble_fold_indices = []\n",
        "\n",
        "    inner_fold_num = 1\n",
        "\n",
        "    # --- INNER cross validation loop ---\n",
        "    for train_index, val_index in cv_inner.split(data_final_selected, y_train1):\n",
        "        print(f\"Starting inner fold {inner_fold_num}\")\n",
        "\n",
        "        # Split data into train and validation sets\n",
        "        X_train, X_val = data_final_selected[train_index], data_final_selected[val_index]\n",
        "        y_train, y_val = y_train1.iloc[train_index], y_train1.iloc[val_index]\n",
        "\n",
        "        # Store classifiers for ensembling\n",
        "        fold_classifiers = []\n",
        "\n",
        "        # Create learning curve plots\n",
        "        i=0\n",
        "        fig, axes = plt.subplots(2, len(classifiers), figsize=(25, 20))\n",
        "\n",
        "        # --- classifier loop ---\n",
        "        for clf_name, clf in classifiers.items():\n",
        "          # initialize list to store results\n",
        "          clf_params.setdefault(clf_name, [])\n",
        "          auc_scores.setdefault(clf_name, [])\n",
        "          auc_scores.setdefault('VotingEnsembleHard', [])\n",
        "          f1_scores.setdefault(clf_name, [])\n",
        "          f1_scores.setdefault('VotingEnsembleHard', [])\n",
        "\n",
        "          ## Hyperparameter optimization\n",
        "          # find best hyperparams for current classifier\n",
        "          grid_search = GridSearchCV(clf, param_grids[clf_name], cv=cv_inner, n_jobs=-1, scoring='f1')\n",
        "          grid_search.fit(X_train, y_train)\n",
        "          clf = grid_search.best_estimator_    # get best estimator\n",
        "          clf_params[clf_name].append(clf)     # store best estimator\n",
        "          print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
        "\n",
        "          ## Calculate F1-score for this classifier\n",
        "          f1 = f1_score(y_val, y_pred)\n",
        "          f1_scores[clf_name].append(f1)  # Store F1-score for this classifier\n",
        "\n",
        "          ### Visualisation\n",
        "          ## Calculate AUC-score\n",
        "          y_pred = clf.predict(X_val)\n",
        "          if hasattr(clf, \"predict_proba\"):\n",
        "              y_score = clf.predict_proba(X_val)[:, 1]\n",
        "          elif hasattr(clf, \"decision_function\"):\n",
        "              y_score = clf.decision_function(X_val)\n",
        "          else:\n",
        "              y_score = y_pred  # fallback if neither method is available\n",
        "\n",
        "          auc = roc_auc_score(y_val, y_score)\n",
        "          auc_scores[clf_name].append(auc)  # Store AUC for this classifier\n",
        "\n",
        "          # Calculate ROC\n",
        "          fpr, tpr, thresholds = roc_curve(y_val, y_score)\n",
        "\n",
        "          # Plot ROC curve for each classifier\n",
        "          axes[0,i].plot(fpr, tpr, label=f'{clf_name} (AUC = {auc:.2f})')\n",
        "          axes[0,i].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
        "          axes[0,i].set_xlabel('False Positive Rate')\n",
        "          axes[0,i].set_ylabel('True Positive Rate')\n",
        "          axes[0,i].set_title(f'ROC Curve - {clf_name}')\n",
        "          axes[0,i].legend()\n",
        "\n",
        "          ## Learning curves\n",
        "          train_sizes, train_scores, val_scores = \\\n",
        "            learning_curve(clf, X_train, y_train,\n",
        "                            cv=cv_inner,n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "          # calculate mean and std of training and validation scores\n",
        "          train_scores_mean = np.mean(train_scores, axis=1)\n",
        "          train_scores_std = np.std(train_scores, axis=1)\n",
        "          val_scores_mean = np.mean(val_scores, axis=1)\n",
        "          val_scores_std = np.std(val_scores, axis=1)\n",
        "\n",
        "          # plot learning curve\n",
        "          axes[1,i].set_title(f'Learning Curve - {clf_name}')\n",
        "          axes[1,i].set_ylim(0.3, 1.01) # Adjust ylim as needed\n",
        "          axes[1,i].set_xlabel(\"Training examples\")\n",
        "          axes[1,i].set_ylabel(\"Score\")\n",
        "          axes[1,i].grid()\n",
        "          axes[1,i].fill_between(train_sizes,train_scores_mean - train_scores_std,\n",
        "                              train_scores_mean + train_scores_std,alpha=0.1,color=\"r\")\n",
        "          axes[1,i].fill_between(train_sizes,val_scores_mean - val_scores_std,val_scores_mean + val_scores_std,alpha=0.1,color=\"g\")\n",
        "          axes[1,i].plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\")\n",
        "          axes[1,i].plot(train_sizes, val_scores_mean, \"o-\", color=\"g\",\n",
        "                      label=\"Cross-validation score\")\n",
        "          axes[1,i].legend(loc=\"best\")\n",
        "\n",
        "          i += 1\n",
        "\n",
        "          ## Selection for ensembling based on f1-score value\n",
        "          if f1 > f1_threshold: # Store high scoring classifiers\n",
        "            fold_classifiers.append((clf_name, clf))\n",
        "\n",
        "      # Out of classifier loop, still in cross-validation loop\n",
        "      ## Ensembling\n",
        "        if len(fold_classifiers) > 1:  # Cannot ensemble 1 classifier\n",
        "            voting_ensemble = VotingClassifier(estimators=fold_classifiers, voting='hard')\n",
        "            voting_ensemble.fit(X_train, y_train)\n",
        "            y_pred = voting_ensemble.predict(X_val)\n",
        "            f1 = f1_score(y_val, y_pred)\n",
        "            f1_scores['VotingEnsembleHard'].append(f1)\n",
        "            ensemble_classifiers.append(voting_ensemble)\n",
        "        else:\n",
        "            clf_names = [name for name, _ in fold_classifiers]\n",
        "            print(f\"Only {len(fold_classifiers)} classifier(s) above F1 threshold in inner fold {inner_fold_num}: {clf_names}. Skipping ensembling.\")\n",
        "\n",
        "        ensemble_fold_indices.append(inner_fold_num)\n",
        "        inner_fold_num += 1\n",
        "\n",
        "    # Out of cross-validation loop\n",
        "    ## Select best ensembling model\n",
        "    if f1_scores['VotingEnsembleHard']:\n",
        "        best_f1 = max(f1_scores['VotingEnsembleHard'])\n",
        "        best_idx = f1_scores['VotingEnsembleHard'].index(best_f1)\n",
        "        best_ensemble = ensemble_classifiers[best_idx]\n",
        "        best_inner_fold = ensemble_fold_indices[best_idx]\n",
        "        print(f'Best ensemble model created in inner fold {best_inner_fold} with F1 score {best_f1:.4f}')\n",
        "    else:\n",
        "        best_ensemble = None\n",
        "        best_f1, best_idx, best_clf_name = max(\n",
        "            [(value, i, key) for key, values in f1_scores.items() for i, value in enumerate(values)],\n",
        "            key=lambda item: item[0])\n",
        "        best_clf = clf_params[best_clf_name][best_idx]\n",
        "        print(f'Best individual classifier: {best_clf_name} from inner fold {best_idx + 1} with F1 score {best_f1:.4f}')\n",
        "\n",
        "    ## Evaluate model on test data\n",
        "    # Change None into median of train data\n",
        "    data_test_clean = X_test.copy()\n",
        "    data_test_clean[:] = imputer.transform(data_test_clean)\n",
        "    # Scaling testdata\n",
        "    X_test_scaled = scaler.transform(data_test_clean)\n",
        "    # Feature selection\n",
        "    X_test_selected = selector.transform(X_test_scaled) # 1. univariate\n",
        "    X_test_final_selected = pca.transform(X_test_selected) # 2. pca\n",
        "\n",
        "    # Ensembling\n",
        "    if best_ensemble:  # if ensemble is created, use for prediction\n",
        "        y_pred_test = best_ensemble.predict(X_test_final_selected)\n",
        "        f1_test_fold = f1_score(y_test, y_pred_test)\n",
        "        f1_test.append(f1_test_fold)\n",
        "        print(f\"Final ensemble model used — F1-score on test data: {f1_test_fold:.4f}\")\n",
        "\n",
        "        print(\"Ensemble includes the following classifiers:\")\n",
        "        for name, estimator in best_ensemble.estimators:\n",
        "            print(f\" - {name}: {estimator}\")\n",
        "    else:  # otherwise use best individual classifier for prediction\n",
        "        y_pred_test = best_clf.predict(X_test_final_selected)\n",
        "        f1_test_fold = f1_score(y_test, y_pred_test)\n",
        "        f1_test.append(f1_test_fold)\n",
        "        print(f\"Final individual model ({best_clf_name}) used — F1-score on test data: {f1_test_fold:.4f}\")\n",
        "\n",
        "    ## Final evaluation\n",
        "    # Calculate precision, recall, and thresholds\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_test)\n",
        "    # Calculate Average Precision (AP)\n",
        "    average_precision = average_precision_score(y_test, y_pred_test)\n",
        "    # Add to list\n",
        "    precision_recall_data = (precision, recall, average_precision)\n",
        "    precision_recall_list.append(precision_recall_data)\n"
      ],
      "metadata": {
        "id": "V6pFCX3R9Mol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "3d39aea5-bb1e-4666-ae89-d340a482c62c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting outer fold 1\n",
            "Normal label: 1\n",
            "Abnormal label: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-89cff40c5015>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mdata_train_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'median'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mdata_train_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m## Scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    449\u001b[0m             )\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             self.statistics_ = self._dense_fit(\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, strategy, missing_values, fill_value)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# Median\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"median\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mmedian_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             \u001b[0;31m# Avoid the warning \"Warning: converting a masked element to nan.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mmedian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedian_masked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/ma/extras.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     return _ureduce(a, func=_median, keepdims=keepdims, axis=axis, out=out,\n\u001b[0m\u001b[1;32m    787\u001b[0m                     overwrite_input=overwrite_input)\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   3762\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEllipsis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindex_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3764\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3766\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/ma/extras.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0masorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0masorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order, endwith, fill_value, stable)\u001b[0m\n\u001b[1;32m   7169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7171\u001b[0;31m         a.sort(axis=axis, kind=kind, order=order, endwith=endwith,\n\u001b[0m\u001b[1;32m   7172\u001b[0m                fill_value=fill_value, stable=stable)\n\u001b[1;32m   7173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, axis, kind, order, endwith, fill_value, stable)\u001b[0m\n\u001b[1;32m   5842\u001b[0m                             fill_value=fill_value, endwith=endwith)\n\u001b[1;32m   5843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5844\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5846\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_shape_base_impl.py\u001b[0m in \u001b[0;36mtake_along_axis\u001b[0;34m(arr, indices, axis)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# use the fancy index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_make_along_axis_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, indx)\u001b[0m\n\u001b[1;32m   3285\u001b[0m             \u001b[0;31m# expect a scalar. It also cannot be of dtype object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3286\u001b[0m             \u001b[0mmout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3287\u001b[0;31m             \u001b[0mscalar_expected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m_is_scalar\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m   3255\u001b[0m         \u001b[0m_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Out of outer cross-validation loop\n",
        "Visualisation"
      ],
      "metadata": {
        "id": "0C4_0LqrzHEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize f-scores for univariate feature selection\n",
        "sorted_scores = np.sort(f_scores)[::-1]  # Sort on descending\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(sorted_scores[:2000])\n",
        "plt.axvline(x=k_best, color='r', linestyle='--', label=f'{k_best} features selected')\n",
        "plt.title('Sorted ANOVA F-scores of features')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('F-score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot variance for PCA\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(cumulative_var, marker='o', label='Cumulative variance')\n",
        "plt.axhline(y=0.99, color='r', linestyle='--', label='99%') # earlier defined 99% variance threshold for PCA\n",
        "plt.axvline(x=pca_components, color='g', linestyle='--', label=f'{pca_components} components')\n",
        "plt.title('Cumulative variance per PCA-component')\n",
        "plt.xlabel('Number of PCA-components')\n",
        "plt.ylabel('Cumulative variance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualize new features with t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42) # Reduce to 2 dimensions for plotting\n",
        "data_tsne = tsne.fit_transform(data_final_selected)\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data_tsne[y_train1 == 0, 0], data_tsne[y_train1 == 0, 1], label='Label 0', marker='o')  # Plot points for label 0\n",
        "plt.scatter(data_tsne[y_train1 == 1, 0], data_tsne[y_train1 == 1, 1], label='Label 1', marker='x')  # Plot points for label 1\n",
        "plt.legend()  # Add a legend to identify the labels\n",
        "plt.title('t-SNE Visualization of Selected Features')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SyMaRFaszPgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance estimate"
      ],
      "metadata": {
        "id": "w_JSeS85zQAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_test_mean = np.mean(f1_test)\n",
        "print(f\"mean F1-score on testdata: {f1_test_mean}\")\n",
        "\n",
        "# confusion matrix\n",
        "metrics.confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=metrics.confusion_matrix(y_test, y_pred_test),\n",
        "                              display_labels=np.unique(y_test)) # Changed to unique labels in filtered_label_test\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# # Plot Precision-Recall curve\n",
        "\n",
        "all_precisions = []\n",
        "all_recalls = []\n",
        "all_average_precisions = []\n",
        "fig, ax = plt.subplots()\n",
        "for i, (precision, recall, average_precision) in enumerate(precision_recall_list):\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_average_precisions.append(average_precision)\n",
        "    # Plot Precision-Recall curve\n",
        "    display = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=average_precision)\n",
        "    display.plot(ax=ax, name=f\"Fold {i+1}\", linestyle='--')  # Plot on the same axes with a label for each fold\n",
        "\n",
        "display = PrecisionRecallDisplay(precision=np.mean(all_precisions, axis=0), recall=np.mean(all_recalls, axis=0), average_precision=np.mean(all_average_precisions))\n",
        "display.plot(ax=ax, name=f\"All Folds\", linestyle='-')  # Plot on the same axes with a label for each fold\n",
        "plt.title('Precision-Recall curve for All Folds')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oxm-PzFnzeav"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}